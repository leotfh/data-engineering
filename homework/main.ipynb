{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd355fb7",
   "metadata": {},
   "source": [
    "# DE Homework - Paul Bichl & Leo Traußnigg\n",
    "### Data from Smart Home - Solar power, UV radiation and Outdoor Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a324c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_sensor_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c63665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dat from csv file\n",
    "def plot_sensor_data(data):\n",
    "    if data['state'] is not None:\n",
    "        data['state'].plot()\n",
    "        plt.title('Sensor Data')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel(data['entity_id'][0])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_duplicates(data, duplicates):\n",
    "    if duplicates > 0:\n",
    "        for _ in range(duplicates):\n",
    "            data = pd.concat([data, data.iloc[0:1]], ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428a4cf-b5dc-47ab-b48b-e57f8ccca56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "\n",
    "    try: \n",
    "        # convert weird date format to actual datetime\n",
    "        data['last_changed'] = pd.to_datetime(data['last_changed'])\n",
    "        \n",
    "        # Define cutoff timestamp\n",
    "        cutoff = pd.Timestamp('2025-03-21T12:00:00.000Z')\n",
    "        \n",
    "        # Filter the data: keep rows where last_changed >= cutoff\n",
    "        data = data[data[\"last_changed\"] >= cutoff].reset_index(drop=True)\n",
    "\n",
    "        # check for duplicate entries\n",
    "        if data.duplicated(subset=['last_changed']).any():\n",
    "            print(\"Duplicate entries found. Removing duplicates.\")\n",
    "            data = data.drop_duplicates(subset=['last_changed']).reset_index(drop=True)\n",
    "        \n",
    "        # Remove rows where 'state' is 'unknown' or 'unavailable'\n",
    "        data = data[~data[\"state\"].isin([\"unknown\", \"unavailable\", \"NaN\", \"nan\"])].reset_index(drop=True)\n",
    "\n",
    "        # Convert string 'state' to number\n",
    "        data['state'] = data['state'].astype('float')\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215b3ad-2987-443b-816f-031d230e3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiation_data = read_sensor_data('data/radiation-history.csv')\n",
    "pv_data = read_sensor_data('data/pv-power-history.csv')\n",
    "temp_data = read_sensor_data('data/outdoor_temp_history.csv')\n",
    "\n",
    "insert_duplicates(radiation_data, 2)\n",
    "\n",
    "radiation_data = clean_data(radiation_data)\n",
    "pv_data = clean_data(pv_data)\n",
    "temp_data = clean_data(temp_data)\n",
    "\n",
    "# plot_sensor_data(radiation_data)\n",
    "# plot_sensor_data(pv_data)\n",
    "# plot_sensor_data(solar_lux_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00878d-7d1a-486b-91e2-d8105c6ed334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on 'last_changed' and clean up the columns\n",
    "\n",
    "merged_data = pv_data.merge(radiation_data, on='last_changed', suffixes=('_pv', '_rad')).merge(temp_data, on='last_changed', suffixes=('', '_temp'))\n",
    "merged_data = merged_data.rename(columns={'state': 'state_temp'})\n",
    "merged_data = merged_data.drop(columns=[col for col in merged_data.columns if col.startswith('entity_id')])\n",
    "\n",
    "print(merged_data)\n",
    "\n",
    "plt.semilogy(merged_data['state_rad'][:100], linewidth='1', linestyle='--')\n",
    "plt.semilogy(merged_data['state_temp'][:100], linewidth='1', linestyle='--')\n",
    "plt.semilogy(merged_data['state_pv'][:100], linewidth='1')\n",
    "\n",
    "plt.title('Sensor Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sensor Values (log scale)')\n",
    "plt.legend([r'Radiation $\\left(\\frac{W}{m^2}\\right)$', 'Outdoor Temp. (°C)', 'PV Power (W)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb8772-f478-4626-9e41-848173e5c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = merged_data.isnull().sum()\n",
    "missing_values += merged_data.isna().sum()\n",
    "print(\"Missing values in merged data:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2703ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Encode the 'state_pv' column as a categorical variable\n",
    "le = LabelEncoder()\n",
    "merged_data['solar_encoded'] = le.fit_transform(merged_data['state_pv'])\n",
    "print(merged_data['solar_encoded'])\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(merged_data[['state_rad', 'state_temp']])\n",
    "print(X_scaled[:5])  # Print first 5 rows of scaled features for verification\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, merged_data['solar_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# print(\"Best parameters found:\", grid_search.best_params_)\n",
    "# print(\"Best RMSE:\", (-grid_search.best_score_)**0.5)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=None, min_samples_split=5, min_samples_leaf=1, n_estimators=50, random_state=42)\n",
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "Y_pred = rf.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\") # (Lower is better)\n",
    "print(f\"MSE: {mse:.2f}\") # (Lower is better)\n",
    "print(f\"RMSE: {rmse:.2f}\") # (Lower is better)\n",
    "print(f\"R²: {r2:.2f}\") # (1 is perfect, 0 is no correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b130d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['state_rad', 'state_temp']  # or your actual feature columns\n",
    "target = 'state_pv'  # the original, unencoded target\n",
    "\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "# Use the same random_state and test_size to get the same split\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Get the first 100 true PV power values from the original, unshuffled data\n",
    "true_pv_power_segment = merged_data['state_pv'][:100].values\n",
    "\n",
    "# Get the corresponding features for these first 100 samples\n",
    "# Ensure 'features' list is defined as ['state_rad', 'state_temp'] or as needed\n",
    "features_list = ['state_rad', 'state_temp'] \n",
    "features_segment = merged_data[features_list][:100]\n",
    "\n",
    "# Scale these features using the already fitted scaler\n",
    "scaled_features_segment = scaler.transform(features_segment)\n",
    "\n",
    "# Make predictions on this segment using the trained model 'rf' \n",
    "predicted_pv_encoded_segment = rf.predict(scaled_features_segment)\n",
    "\n",
    "# Inverse transform the predictions to the original PV power scale (using 'le' from cell d3be6bf1)\n",
    "predicted_pv_power_segment = le.inverse_transform(predicted_pv_encoded_segment)\n",
    "\n",
    "# Now plot\n",
    "#plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_pv_power_segment, label='True PV Power (W)', alpha=0.7, linewidth=2)\n",
    "plt.plot(predicted_pv_power_segment, label='Predicted PV Power (W)', alpha=0.8, linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.title('Comparison')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('PV Power (W)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load the model\n",
    "import joblib\n",
    "joblib.dump(rf, 'solar_model.pkl')\n",
    "rf_loaded = joblib.load('solar_model.pkl')\n",
    "\n",
    "# Predict using the loaded model\n",
    "Y_pred_loaded = rf_loaded.predict(X_test)\n",
    "print(\"Predictions from loaded model:\")\n",
    "# print(Y_pred_loaded)\n",
    "print(classification_report(Y_test, Y_pred_loaded))\n",
    "for pred, true in zip(Y_pred_loaded, Y_test):\n",
    "    print(f\"Predicted: {pred}, Actual: {true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da6dab",
   "metadata": {},
   "source": [
    "Save to SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://user:passwd@localhost/smart_home\")\n",
    "\n",
    "# Save merged_data to MySQL table \"processed_sensor_data\"\n",
    "merged_data.to_sql('processed_sensor_data', engine, if_exists='replace', index=False, method='multi')\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658b1fd",
   "metadata": {},
   "source": [
    "Create SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "\n",
    "# 1. Select all rows where PV power is greater than 10kW\n",
    "queries.append(\"SELECT * FROM processed_sensor_data WHERE state_pv > 10000\")\n",
    "\n",
    "# 2. Find the day with the highest average PV power (whole day average)\n",
    "queries.append(\"\"\"\n",
    "SELECT day, avg_pv_power FROM (\n",
    "    SELECT DATE(last_changed) AS day, AVG(state_pv) AS avg_pv_power\n",
    "    FROM processed_sensor_data\n",
    "    GROUP BY day\n",
    ") AS daily_avg\n",
    "ORDER BY avg_pv_power DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# 3. Find the average radiation and PV power for each day\n",
    "queries.append(\"\"\"\n",
    "SELECT DATE(last_changed) as day, AVG(state_rad) as avg_radiation, AVG(state_pv) as avg_pv_power\n",
    "FROM processed_sensor_data\n",
    "GROUP BY day\n",
    "ORDER BY day\n",
    "\"\"\")\n",
    "\n",
    "# 4. Get the maximum PV power recorded and the corresponding timestamp\n",
    "queries.append(\"\"\"\n",
    "SELECT last_changed, state_pv\n",
    "FROM processed_sensor_data\n",
    "WHERE state_pv = (SELECT MAX(state_pv) FROM processed_sensor_data)\n",
    "\"\"\")\n",
    "\n",
    "# 5. Select all rows where radiation is below 500 and PV power is above 500\n",
    "queries.append(\"SELECT * FROM processed_sensor_data WHERE state_rad < 50 AND state_pv > 500\")\n",
    "\n",
    "# Example: execute and print the queries using the SQLAlchemy engine\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"Query {i}:\\n{query}\\n\")\n",
    "    result = pd.read_sql_query(query, engine)\n",
    "    print(result.head())\n",
    "    print(\"\\n\" + \"_________________________________________\" + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
